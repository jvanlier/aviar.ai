{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"~/data/split-v3-random-nongray\").expanduser() \n",
    "BASE_PATH = Path(\"~/models/model-from-scratch-dropout-earlier/\").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(str(BASE_PATH / \"export\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with (BASE_PATH / \"export-tflite.tflite\").open('wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = tf.Variable([[[0.74136317, 0.73086095, 0.74473]]])\n",
    "IMAGE_SIZE = 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator that provides a representative dataset\n",
    "def representative_data_gen(sample_size=1000):\n",
    "    sample_paths = np.random.choice(list((DATA_PATH / \"train\").glob(\"*/*.jpeg\")), \n",
    "                                    size=sample_size)\n",
    "    \n",
    "    for img_path in sample_paths:\n",
    "        image = tf.io.read_file(str(img_path))\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "        image = tf.cast(image / 128., tf.float32) - mean\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        yield [image]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# And this sets the representative dataset so we can quantize the activations\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with (BASE_PATH / \"export-tflite_quant.tflite\").open('wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3280 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 336\n",
    "\n",
    "BATCH_SIZE = 90\n",
    "RESCALE = 1/128\n",
    "\n",
    "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     rescale=RESCALE, \n",
    "#     featurewise_center=True,\n",
    "#     width_shift_range=.025,\n",
    "#     height_shift_range=.025,\n",
    "#     brightness_range=(.8, 1.2),\n",
    "#     zoom_range=.025,\n",
    "# )\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     DATA_PATH / \"train\",\n",
    "#     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "#     batch_size=BATCH_SIZE)\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=RESCALE,\n",
    "    featurewise_center=True\n",
    ")\n",
    "val_datagen.mean = mean\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    DATA_PATH / \"valid\",\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model accuracy: 97.778%\n"
     ]
    }
   ],
   "source": [
    "batch_images, batch_labels = next(val_generator)\n",
    "\n",
    "logits = model(batch_images)\n",
    "prediction = np.argmax(logits, axis=1)\n",
    "truth = np.argmax(batch_labels, axis=1)\n",
    "\n",
    "keras_accuracy = tf.keras.metrics.Accuracy()\n",
    "keras_accuracy(prediction, truth)\n",
    "\n",
    "print(\"Raw model accuracy: {:.3%}\".format(keras_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, input):\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    tensor_index = input_details['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = input\n",
    "    # NOTE: This model uses float inputs, but if inputs were uint8,\n",
    "    # we would quantize the input like this:\n",
    "    #   scale, zero_point = input_details['quantization']\n",
    "    #   input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
    "\n",
    "def classify_image(interpreter, input):\n",
    "    set_input_tensor(interpreter, input)\n",
    "    interpreter.invoke()\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    output = interpreter.get_tensor(output_details['index'])\n",
    "    # NOTE: This model uses float outputs, but if outputs were uint8,\n",
    "    # we would dequantize the results like this:\n",
    "    #   scale, zero_point = output_details['quantization']\n",
    "    #   output = scale * (output - zero_point)\n",
    "    top_1 = np.argmax(output)\n",
    "    return output[0][0], top_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quant TF Lite accuracy: 85.556%\n",
      "CPU times: user 3min 50s, sys: 25.4 ms, total: 3min 50s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "interpreter = tf.lite.Interpreter(str(BASE_PATH / \"export-tflite_quant.tflite\"))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Collect all inference predictions in a list\n",
    "batch_prediction = []\n",
    "batch_truth = np.argmax(batch_labels, axis=1)\n",
    "\n",
    "for i in range(len(batch_images)):\n",
    "    prob, klass = classify_image(interpreter, batch_images[i])\n",
    "    batch_prediction.append(prediction)\n",
    "\n",
    "# Compare all predictions to the ground truth\n",
    "tflite_accuracy = tf.keras.metrics.Accuracy()\n",
    "tflite_accuracy(batch_prediction, batch_truth)\n",
    "print(\"Quant TF Lite accuracy: {:.3%}\".format(tflite_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = np.random.choice(list((DATA_PATH / \"train\").glob(\"*/*.jpeg\")), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BirdHome\n",
      "Label: BirdHome, pred: BirdHome, prob: 0.996\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "KERAS_IMG_SIZE = (336, 336)\n",
    "KERAS_RESCALE = 1/128\n",
    "KERAS_MEAN = np.array([[[0.74136317, 0.73086095, 0.74473]]], dtype=np.float32)\n",
    "\n",
    "img_path = sample_paths[3]\n",
    "img = plt.imread(img_path)\n",
    "img_small = cv2.resize(img, KERAS_IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "img_small_norm = (img_small * KERAS_RESCALE) - KERAS_MEAN\n",
    "prob, klass = classify_image(interpreter, img_small_norm)\n",
    "label = img_path.parent.name\n",
    "klass_str = \"BirdHome\" if klass == 0 else \"BirdRoaming\"\n",
    "print(f\"Label: {img_path.parent.name}, pred: {klass_str}, prob: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9777778>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_accuracy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9777778>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_accuracy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge TPU Compiler version 2.1.302470888\n",
      "\n",
      "Model compiled successfully in 352 ms.\n",
      "\n",
      "Input model: /home/jvlier/models/model-from-scratch-dropout-earlier/export-tflite_quant.tflite\n",
      "Input size: 410.00KiB\n",
      "Output model: export-tflite_quant_edgetpu.tflite\n",
      "Output size: 644.80KiB\n",
      "On-chip memory used for caching model parameters: 79.50KiB\n",
      "On-chip memory remaining for caching model parameters: 5.42MiB\n",
      "Off-chip memory used for streaming uncached model parameters: 361.12KiB\n",
      "Number of Edge TPU subgraphs: 1\n",
      "Total number of operations: 13\n",
      "Operation log: export-tflite_quant_edgetpu.log\n",
      "\n",
      "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
      "Number of operations that will run on Edge TPU: 11\n",
      "Number of operations that will run on CPU: 2\n",
      "See the operation log file for individual operation details.\n"
     ]
    }
   ],
   "source": [
    "! edgetpu_compiler $BASE_PATH/export-tflite_quant.tflite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
